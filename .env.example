# Google Drive API (for document ingestion only)
GOOGLE_APPLICATION_CREDENTIALS=/secrets/sa.json
ROOT_FOLDER_ID=YOUR_DRIVE_FOLDER_ID

# Database
DB_URL=postgresql+psycopg://rag_user:rag_password@localhost:5432/rag_db

# Embedding Configuration (Local Open Source - No API Key)
# Recommended models:
#   - intfloat/multilingual-e5-large (1024 dim, best quality, multilingual including Finnish)
#   - BAAI/bge-large-en-v1.5 (1024 dim, excellent quality, English)
#   - sentence-transformers/all-mpnet-base-v2 (768 dim, good quality, English)
#   - sentence-transformers/all-MiniLM-L6-v2 (384 dim, fast, decent quality)
EMBEDDING_MODEL=intfloat/multilingual-e5-large
EMBEDDING_DIMENSION=1024

# LLM Configuration
# Choose provider: "ollama" (default), "openai" (for LM Studio, OpenAI, etc.), or "gemini"
LLM_PROVIDER=ollama

# Ollama Configuration (if LLM_PROVIDER=ollama)
# Install Ollama from https://ollama.com
# Then: ollama pull mistral (or llama3.1, phi3, etc.)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=mistral

# OpenAI-compatible API Configuration (if LLM_PROVIDER=openai)
# For LM Studio (https://lmstudio.ai):
#   1. Download and install LM Studio
#   2. Download a model (e.g., Mistral 7B)
#   3. Go to "Local Server" tab and click "Start Server"
#   4. Use settings below (use host.docker.internal if running in Docker):
OPENAI_API_BASE=http://localhost:1234/v1
OPENAI_API_KEY=lm-studio
OPENAI_MODEL=local-model
# For OpenAI:
#   OPENAI_API_BASE=https://api.openai.com/v1
#   OPENAI_API_KEY=sk-your-key-here
#   OPENAI_MODEL=gpt-4
# Note: When running in Docker, replace localhost with host.docker.internal

# Gemini API Configuration (if LLM_PROVIDER=gemini)
# Get API key from https://aistudio.google.com/app/apikey
GEMINI_API_KEY=your-gemini-api-key-here
GEMINI_MODEL=gemini-1.5-flash
# Models: gemini-1.5-flash (fast, cheap), gemini-1.5-pro (best quality), gemini-1.0-pro

# Reranker Configuration (BGE - Local Open Source - No API Key)
# BAAI/bge-reranker-v2-m3 supports 100+ languages
# Alternative: cross-encoder/ms-marco-MiniLM-L-6-v2 (lightweight, English)
RERANKER_MODEL=BAAI/bge-reranker-v2-m3

# Chunking Configuration
MAX_CHUNK_TOKENS=400
CHUNK_OVERLAP=60

# Retrieval Configuration
TOPK_CANDIDATES=50
TOPK_CONTEXT=8
ENABLE_HYDE=false
ENABLE_MULTI_QUERY=true
MULTI_QUERY_COUNT=3

# Redis (for Celery background jobs)
REDIS_URL=redis://localhost:6379/0

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
LOG_LEVEL=INFO
